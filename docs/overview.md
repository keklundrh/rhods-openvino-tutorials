
# Intel OpenVINO Overview 

## Introduction 

Intel OpenVINO optimizes our models to get the most performance possible without significant accuracy sacrifices. In other words, it optimizes your model's predictions for the *intended Intel inference device*. It eliminates the discrepancy between high-end hardware used in training and low-end, constrained deployment devices. Instead of making large accuracy sacrifices for adequate performance, OpnVINO *optimizes performance* by reducing precision with *minimal changes to accuracy*, a process called *quantization*. 

For example, a double precision representation of pi is *3.141592653589793* while single precision is *3.14159265*, 16 digits vs 8. Why calculate a precise representation if you donâ€™t have to?

Additionally, Red Hat OpenShift Data Science runs on Intel hardware. Consider using OpenVINO as your default serving engine.

## How it works 


## Optimization details 


## Benefits 


## References 

* [OpenVINO Docs](https://docs.openvino.ai/latest/index.html)
